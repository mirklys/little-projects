{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPACrtiAXNfiun3hCLhGsw9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mirklys/little-projects/blob/main/thesis/testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install Box2D\n",
        "!pip3 install box2d-py\n",
        "!pip3 install gym[all]\n",
        "!pip3 install gym[Box_2D]"
      ],
      "metadata": {
        "id": "WMZs7R9ZI3rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable_baselines\n",
        "!pip install stable_baselines3"
      ],
      "metadata": {
        "id": "7S4-NTFWI9uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "j0eb_a4oI_Ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import os\n",
        "import numpy as np\n",
        "import torch as th\n",
        "from torch import nn\n",
        "from torch.distributions.bernoulli import Bernoulli\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import time\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "from stable_baselines3.common.utils import get_device\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines.common import set_global_seeds, make_vec_env\n",
        "from stable_baselines3.common.monitor import Monitor"
      ],
      "metadata": {
        "id": "CBYOOEBRJAgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8ubm27tJFfZ",
        "outputId": "8b622b34-5761-4241-90dc-24ee8c85369b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = th.device('cuda' if th.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cp-MuFqQZf1l",
        "outputId": "c535251e-dc57-4058-aa26-56bfa9970f71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_BASE = '/content/gdrive/MyDrive/Thesis Project'\n",
        "PATH_DATA = os.path.join(PATH_BASE, 'data/')\n",
        "PATH_NETWORKS = os.path.join(PATH_BASE, 'networks/')\n",
        "PATH_PLOTS = os.path.join(PATH_BASE, 'plots/')\n",
        "PATH_RESULTS = os.path.join(PATH_BASE, 'results/')\n",
        "PATH_LOGS = os.path.join(PATH_BASE, 'logs/')\n",
        "os.chdir(PATH_BASE)"
      ],
      "metadata": {
        "id": "x80V7H8yJHSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_network(game:str, size:int, dropout:float) -> str:\n",
        "    title = \"{}.{}x{}.dropout_{}\".format(game, size, size, dropout)\n",
        "    env = make_vec_env(game, n_envs=10, seed=0, vec_env_cls=DummyVecEnv)\n",
        "    model = PPO.load(os.path.join(PATH_NETWORKS, game, title))\n",
        "    rew, std = evaluate_policy(model, env, n_eval_episodes=10)\n",
        "\n",
        "    return \"Networks cumulative reward {:.2f} ±{:.2f}\".format(rew, std)"
      ],
      "metadata": {
        "id": "1x6q_2jGJKgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_network('CartPole-v1', 128, 0.8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "bP7DFzUEKn_O",
        "outputId": "50e5c839-33b6-4052-cdcc-1ad566d72515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Networks cumulative reward 500.00 ±0.00'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_title(title: str) -> tuple:\n",
        "    splitted = title.split(\".\")\n",
        "    game = splitted[0]\n",
        "    size = splitted[1].split(\"x\")[0]\n",
        "    dropout = float(title.split(\"_\")[1])\n",
        "    return game, size, dropout"
      ],
      "metadata": {
        "id": "7YeDUmNjUrNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_with_mask(model, percent_to_mask, env) -> tuple:\n",
        "    rewards = []\n",
        "    for i in range(20): # repeat over multiple random masks\n",
        "        model.policy.features_extractor.mask_units(percent_to_mask=percent_to_mask)\n",
        "        mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
        "        rewards.append(mean_reward)\n",
        "    return np.round(np.mean(rewards), 3), np.round(np.std(rewards), 3)"
      ],
      "metadata": {
        "id": "2VyWN8fsUXP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import warnings\n",
        "import pickle\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "num_training_steps = 10\n",
        "overtrained_evals = []\n",
        "inc = 0.05\n",
        "environment = 'LunarLander-v2'\n",
        "nets = os.listdir(os.path.join(PATH_NETWORKS, environment)) #retrieved all the network names of an environment\n",
        "evals = []\n",
        "env = make_vec_env(environment, n_envs=10, seed=0, vec_env_cls=DummyVecEnv) # created the environment\n",
        "max_rew = 180 # i reduced the amount of max_reward because I did not want to be stuck trying reach 200, so I was fine with 180 too.\n",
        "for net in nets:\n",
        "    game, size, dropout = parse_title(net)\n",
        "    print(size, dropout)\n",
        "    model = PPO.load(os.path.join(PATH_NETWORKS, game, net), device=device)\n",
        "    model.set_env(env)\n",
        "    # i was apprehensive to mask and retrain the same network, it coudl have affected the initial testing\n",
        "    model_msk = PPO.load(os.path.join(PATH_NETWORKS, game, net), device=device)\n",
        "    model_msk.set_env(env)\n",
        "    model.policy.features_extractor.training = False # disabling training\n",
        "    print(\"Masking size: \")\n",
        "    name = 'f'\n",
        "    mean_rews, std_rews = [], []\n",
        "    for mask in np.arange(0., 1, inc):\n",
        "        name = net\n",
        "        print(mask, end=\" \")\n",
        "        eval = eval_with_mask(model, percent_to_mask=float(mask), env=env) # inital testing\n",
        "        print(eval, end='\\n')\n",
        "        mean_rews.append(eval[0])\n",
        "        std_rews.append(eval[1])\n",
        "        rew = eval[0]\n",
        "        std = eval[1]\n",
        "        if rew < max_rew:\n",
        "            print(\"we need some more training for it!\")\n",
        "            name = net\n",
        "            model_msk.policy.features_extractor.job = 'train_masked' # enabling masked_training, does not pass through the dropout layer\n",
        "            model_msk.policy.features_extractor.percent_to_mask = float(mask)\n",
        "            model_msk.policy.features_extractor.mask_units(percent_to_mask=float(mask))\n",
        "\n",
        "            total_further_train_steps = 0\n",
        "\n",
        "            print(name, mask)\n",
        "            print(\"steps: \", total_further_train_steps, \"rew: \", rew, \"+-\", std)\n",
        "            while rew < max_rew:\n",
        "                model_msk.learn(num_training_steps)\n",
        "                rew, std = evaluate_policy(model_msk, env, n_eval_episodes=10)\n",
        "                total_further_train_steps += num_training_steps\n",
        "                print(\"steps: \", total_further_train_steps, \". rew: \", rew, \"+- \", std)\n",
        "                if np.abs(std/rew) < 0.1 and rew < 30: break\n",
        "                elif np.abs(std/rew) < .03 and rew > max_rew: break\n",
        "            \n",
        "            name += '.mask_{}.overtrained'.format(mask)\n",
        "            model_msk.save(os.path.join(PATH_NETWORKS, game, 'further_trained', name))\n",
        "            print('saved freshly further trained model \\n')\n",
        "            overtrained_evals.append([name, rew, std, total_further_train_steps])\n",
        "            # I saved my results every iteration \n",
        "            # because google colab stops running after around 6 hours if I don't do anything there,\n",
        "            # so I continued from where it stopped, saw it in outputs\n",
        "            with open(os.path.join(PATH_RESULTS, \"overtrained_evals_LunarLander-v2.pickle\"), \"wb\") as f:\n",
        "                pickle.dump(overtrained_evals, f)\n",
        "                print(\"we have overwritten overtrained_evals_LunarLander-v2.pickle\")\n",
        "    print('\\n')\n",
        "    \n",
        "    evals.append([game, size, dropout, mean_rews, std_rews])\n",
        "    with open(os.path.join(PATH_RESULTS, \"evals_LunarLander-v2.pickle\"), \"wb\") as f:\n",
        "        pickle.dump(evals, f)\n",
        "        print(\"we have overwritten evals_LunarLander-v2.pickle\")\n"
      ],
      "metadata": {
        "id": "mEZFYdncT9_W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}