{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "testing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPMEjWFz+YF2jF0VzLNwc2W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mirklys/little-projects/blob/main/thesis/testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNBlKvIoZpe1",
        "outputId": "9e99cacf-8352-4718-ff06-4e9d2003df5c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Apr 20 14:36:46 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install Box2D\n",
        "!pip3 install box2d-py\n",
        "!pip3 install gym[all]\n",
        "!pip3 install gym[Box_2D]"
      ],
      "metadata": {
        "id": "WMZs7R9ZI3rV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable_baselines\n",
        "!pip install stable_baselines3"
      ],
      "metadata": {
        "id": "7S4-NTFWI9uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "j0eb_a4oI_Ai"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import os\n",
        "import numpy as np\n",
        "import torch as th\n",
        "from torch import nn\n",
        "from torch.distributions.bernoulli import Bernoulli\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import time\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "from stable_baselines3.common.utils import get_device\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines.common import set_global_seeds, make_vec_env\n",
        "from stable_baselines3.common.monitor import Monitor"
      ],
      "metadata": {
        "id": "CBYOOEBRJAgD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8ubm27tJFfZ",
        "outputId": "c4ba333c-34d9-467a-c65e-b39dcf4a76d7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = th.device('cuda' if th.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cp-MuFqQZf1l",
        "outputId": "5f5d20da-f9b6-4571-e986-79e927205e2b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_BASE = '/content/gdrive/MyDrive/Thesis Project'\n",
        "PATH_DATA = os.path.join(PATH_BASE, 'data/')\n",
        "PATH_NETWORKS = os.path.join(PATH_BASE, 'networks/')\n",
        "PATH_PLOTS = os.path.join(PATH_BASE, 'plots/')\n",
        "PATH_RESULTS = os.path.join(PATH_BASE, 'results/')\n",
        "PATH_LOGS = os.path.join(PATH_BASE, 'logs/')\n",
        "os.chdir(PATH_BASE)"
      ],
      "metadata": {
        "id": "x80V7H8yJHSC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_network(game:str, size:int, dropout:float) -> str:\n",
        "    title = \"{}.{}x{}.dropout_{}\".format(game, size, size, dropout)\n",
        "    env = make_vec_env(game, n_envs=10, seed=0, vec_env_cls=DummyVecEnv)\n",
        "    model = PPO.load(os.path.join(PATH_NETWORKS, game, title))\n",
        "\n",
        "    rew, std = evaluate_policy(model, env, n_eval_episodes=10)\n",
        "\n",
        "    return \"Networks cumulative reward {:.2f} ±{:.2f}\".format(rew, std)"
      ],
      "metadata": {
        "id": "1x6q_2jGJKgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_network('BipedalWalker-v3', 128, 0.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "bP7DFzUEKn_O",
        "outputId": "2e761781-eafa-4a23-b4b6-afa280fee8ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Networks cumulative reward 317.78 ±0.60'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_network('BipedalWalker-v3', 128, 0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "Y2J8WaTmK0d-",
        "outputId": "fee1adc9-028b-44b6-d6a6-c51fc6cf2352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Networks cumulative reward 200.50 ±135.86'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_title(title: str) -> tuple:\n",
        "    splitted = title.split(\".\")\n",
        "    game = splitted[0]\n",
        "    size = splitted[1].split(\"x\")[0]\n",
        "    dropout = float(title.split(\"_\")[1])\n",
        "    return game, size, dropout"
      ],
      "metadata": {
        "id": "7YeDUmNjUrNO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_with_mask(model, percent_to_mask, env):\n",
        "    rewards = []\n",
        "    for i in range(20): # repeat over multiple random masks)\n",
        "        model.policy.features_extractor.mask_units(percent_to_mask=percent_to_mask)\n",
        "        mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
        "        rewards.append(mean_reward)\n",
        "    return np.round(np.mean(rewards), 3), np.round(np.std(rewards), 3)"
      ],
      "metadata": {
        "id": "2VyWN8fsUXP7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "inc = 0.05\n",
        "nets = os.listdir(os.path.join(PATH_NETWORKS, 'CartPole-v1'))\n",
        "evals = defaultdict(list)\n",
        "env = make_vec_env('CartPole-v1', n_envs=10, seed=0, vec_env_cls=DummyVecEnv)\n",
        "for net in nets:\n",
        "    print(net)\n",
        "    game, size, dropout = parse_title(net)\n",
        "    model = PPO.load(os.path.join(PATH_NETWORKS, game, net), device=device)\n",
        "    model.set_env(env)\n",
        "    model.policy.features_extractor.training = False\n",
        "    print(\"Masking size: \")\n",
        "    mean_rews, std_rews = [], []\n",
        "    for mask in np.arange(0., 1.+inc, inc):\n",
        "        print(mask, end=\" \")\n",
        "        eval = eval_with_mask(model, percent_to_mask=float(mask), env=env)\n",
        "        print(eval, end='\\n')\n",
        "        mean_rews.append(eval[0])\n",
        "        std_rews.append(eval[1])\n",
        "    print('\\n')\n",
        "    evals[(game, size, dropout)].append((mean_rews, std_rews))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEZFYdncT9_W",
        "outputId": "cfa4899a-2234-46a5-80e7-e10b9c857886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CartPole-v1.128x128.dropout_0.0\n",
            "Masking size: \n",
            "0.0 (500.0, 0.0)\n",
            "0.05 (500.0, 0.0)\n",
            "0.1 (500.0, 0.0)\n",
            "0.15000000000000002 (500.0, 0.0)\n",
            "0.2 (500.0, 0.0)\n",
            "0.25 (500.0, 0.0)\n",
            "0.30000000000000004 (500.0, 0.0)\n",
            "0.35000000000000003 (500.0, 0.0)\n",
            "0.4 (500.0, 0.0)\n",
            "0.45 (500.0, 0.0)\n",
            "0.5 (500.0, 0.0)\n",
            "0.55 (500.0, 0.0)\n",
            "0.6000000000000001 (500.0, 0.0)\n",
            "0.65 (500.0, 0.0)\n",
            "0.7000000000000001 (500.0, 0.0)\n",
            "0.75 (500.0, 0.0)\n",
            "0.8 (500.0, 0.0)\n",
            "0.8500000000000001 (480.374, 85.548)\n",
            "0.9 (474.385, 65.125)\n",
            "0.9500000000000001 (403.878, 147.917)\n",
            "1.0 (9.381, 0.068)\n",
            "\n",
            "\n",
            "CartPole-v1.128x128.dropout_0.2\n",
            "Masking size: \n",
            "0.0 (500.0, 0.0)\n",
            "0.05 (500.0, 0.0)\n",
            "0.1 (500.0, 0.0)\n",
            "0.15000000000000002 (500.0, 0.0)\n",
            "0.2 (500.0, 0.0)\n",
            "0.25 (500.0, 0.0)\n",
            "0.30000000000000004 (500.0, 0.0)\n",
            "0.35000000000000003 (500.0, 0.0)\n",
            "0.4 (500.0, 0.0)\n",
            "0.45 (500.0, 0.0)\n",
            "0.5 (500.0, 0.0)\n",
            "0.55 (500.0, 0.0)\n",
            "0.6000000000000001 (500.0, 0.0)\n",
            "0.65 (500.0, 0.0)\n",
            "0.7000000000000001 (500.0, 0.0)\n",
            "0.75 (500.0, 0.0)\n",
            "0.8 (500.0, 0.0)\n",
            "0.8500000000000001 (500.0, 0.0)\n",
            "0.9 (500.0, 0.0)\n",
            "0.9500000000000001 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open(os.path.join(PATH_RESULTS, \"evaluations_cartpole.pickle\"), \"wb\") as f:\n",
        "    pickle.dump(evals, f)"
      ],
      "metadata": {
        "id": "ClLV5CJCTLsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_net_name(game, size, dropout):\n",
        "    return \"{}.{}x{}.dropout_{}\".format(game, size, size, dropout)"
      ],
      "metadata": {
        "id": "4a7iP9N2liML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_training_steps = 1000\n",
        "overtrained_evals = defaultdict(list)\n",
        "for net, eval in evals.items():\n",
        "    game, size, dropout = net\n",
        "    masks = list(np.arange(0., 1.+inc, inc))\n",
        "    for i, e in enumerate(eval):\n",
        "        rew, std = e\n",
        "        if rew < 500:\n",
        "            name = generate_net_name(game, size, dropout)\n",
        "            model = PPO.load(os.path.join(PATH_NETWORKS, game, name), device=device)\n",
        "            mask = masks[i]\n",
        "            model.set_env(env)\n",
        "            model.policy.features_extractor.job = 'train_masked'\n",
        "            model.policy.features_extractor.percent_to_mask = mask\n",
        "            model.policy.features_extractor.mask_units(percent_to_mask=mask)\n",
        "\n",
        "            total_further_train_steps = 0\n",
        "\n",
        "            print(name)\n",
        "            print(\"steps: \", total_further_train_steps, \". rew: \", rew, \"+- \", std)\n",
        "            while rew < 500:\n",
        "                model.learn(num_training_steps)\n",
        "                rew, std = evaluate_policy(model, env, n_eval_episodes=100)\n",
        "                total_further_train_steps = num_training_steps\n",
        "                print(\"steps: \", total_further_train_steps, \". rew: \", rew, \"+- \", std)\n",
        "                if std/rew < 0.05: break\n",
        "            \n",
        "            name += '.mask_{}.overtrained'.format(mask)\n",
        "            overtrained_evals[name].append((rew, std, total_further_train_steps))\n"
      ],
      "metadata": {
        "id": "arLL9H6ajyty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(PATH_RESULTS, \"evaluations_overtrained_cartpole.pickle\"), \"wb\") as f:\n",
        "    pickle.dump(overtrained_evals, f)"
      ],
      "metadata": {
        "id": "fwikJxZJrt7d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}