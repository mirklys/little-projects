{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mirklys/little-projects/blob/main/thesis/training_full.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLt7AvqyrQOm"
      },
      "outputs": [],
      "source": [
        "!pip3 install Box2D\n",
        "!pip3 install box2d-py\n",
        "!pip3 install gym[all]\n",
        "!pip3 install gym[Box_2D]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-g0NopTrp47"
      },
      "outputs": [],
      "source": [
        "!pip install stable_baselines\n",
        "!pip install stable_baselines3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wytxwkxCrrVP"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEy3XKtCruqL"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import os\n",
        "import numpy as np\n",
        "import torch as th\n",
        "from torch import nn\n",
        "from torch.distributions.bernoulli import Bernoulli\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import time\n",
        "\n",
        "from stable_baselines3 import PPO, A2C, SAC\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "from stable_baselines3.common.utils import get_device\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines.common import set_global_seeds, make_vec_env\n",
        "from stable_baselines3.common.monitor import Monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3csfnd3-rwGZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21-9cWTtrx3B"
      },
      "outputs": [],
      "source": [
        "PATH_BASE = '/content/gdrive/MyDrive/Thesis Project'\n",
        "PATH_DATA = os.path.join(PATH_BASE, 'data/')\n",
        "PATH_NETWORKS = os.path.join(PATH_BASE, 'networks/')\n",
        "PATH_PLOTS = os.path.join(PATH_BASE, 'plots/')\n",
        "PATH_RESULTS = os.path.join(PATH_BASE, 'results/')\n",
        "PATH_LOGS = os.path.join(PATH_BASE, 'logs/')\n",
        "os.chdir(PATH_BASE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acU7wE3MUpDT",
        "outputId": "9a79745b-dd1e-4170-d4d2-060af5f98749"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "dev = th.device('cuda' if th.cuda.is_available() else 'cpu') # i was training on GPU because it's faster\n",
        "dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdKKbQeqr10I"
      },
      "outputs": [],
      "source": [
        "class MaskedMLP(BaseFeaturesExtractor):\n",
        "    def __init__(self, observation_space, dropout_param=0.0, percent_to_mask=0.0, size=32, job='train'):\n",
        "        assert job == 'train' or job == 'train_masked',\\\n",
        "            \"This class can only be initialized for jobs: train, train_masked\"\n",
        "        \"\"\"\n",
        "            This class creates an MLP network where training type can be defined (regular or masked).\n",
        "            In the masked training, dropout is excluded.\n",
        "            l1_size: first layer size\n",
        "            l2_size: second layer size\n",
        "            job: type of training, 'train' || 'train_masked'\n",
        "            dropout_param: dropout size\n",
        "            percent_to_mask: masking percentage, how much the network should be damaged\n",
        "        \"\"\"\n",
        "        self.l1_size = size\n",
        "        self.l2_size = size\n",
        "        self.job = job\n",
        "        super(MaskedMLP, self).__init__(observation_space, self.l2_size)\n",
        "\n",
        "        self.dropout_param = dropout_param\n",
        "        self.percent_to_mask = percent_to_mask\n",
        "\n",
        "        input_size = observation_space.shape[0]\n",
        "\n",
        "        self.linear1 = nn.Linear(input_size, self.l1_size)\n",
        "        self.linear2 = nn.Linear(self.l1_size, self.l2_size)\n",
        "        self.elu = nn.ELU()\n",
        "        self.dropout = nn.Dropout(p=self.dropout_param)\n",
        "\n",
        "        self.mask_units(self.percent_to_mask)\n",
        "\n",
        "    def mask_units(self, percent_to_mask):\n",
        "        self.mask_distribution = Bernoulli(th.tensor([1.0-percent_to_mask]*self.l2_size))   \n",
        "        self.mask = self.mask_distribution.sample()\n",
        "\n",
        "    def forward(self, observations):\n",
        "        x = self.linear1(observations)\n",
        "        x = self.elu(x)\n",
        "        x = self.linear2(x)\n",
        "        if self.job == 'train': x = self.dropout(x)\n",
        "        l2 = self.elu(x)\n",
        "\n",
        "        if not self.training or self.job == 'train_masked': # second layer is masked during testing or masked training\n",
        "            self.mask = self.mask.to(l2.device)\n",
        "            l2 = l2*self.mask\n",
        "\n",
        "\n",
        "        return l2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXYhlE9u0dMG"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Training environmets\n",
        "\"\"\"\n",
        "game = 'LunarLander-v2'\n",
        "#game = 'CartPole-v1'\n",
        "steps = int(3e5)\n",
        "max_rew = 200 # trained until the maximum reward. I looked up online, LunarLander hardly reaches more way more than 200, and CartPole does not exceed 500 reward\n",
        "with open(os.path.join(PATH_LOGS, \"{}.txt\".format(game)), \"w\") as f:\n",
        "    f.write(\"Started training {} models for different number of steps \\n\".format(game))\n",
        "    for size in [128, 256, 512, 1024]:\n",
        "        for dropout in np.arange(0, 1.05, 0.05):\n",
        "            num_training_steps = steps\n",
        "            rew = 0\n",
        "            policy_kwargs = dict(\n",
        "                features_extractor_class=MaskedMLP,\n",
        "                features_extractor_kwargs=dict(dropout_param=dropout, size=size, job='train')\n",
        "            )\n",
        "            env = make_vec_env(game, n_envs=10, seed=0, vec_env_cls=DummyVecEnv) # creating environment\n",
        "            model = PPO('MlpPolicy', env, verbose=0,\n",
        "                        policy_kwargs = policy_kwargs, device=dev, batch_size=128, n_epochs=32, learning_rate=2e-5) #initalizing network\n",
        "            d = get_device()\n",
        "            f.write(\"Training {} {} model for {} steps \\n\".format(dropout, size, num_training_steps))\n",
        "            t = time.process_time()\n",
        "            while rew < max_rew:\n",
        "                model.learn(num_training_steps)\n",
        "                model_save_title = \"{}.{}x{}.dropout_{}\".format(game, size, size, dropout)\n",
        "                model.save(os.path.join(PATH_NETWORKS, game, model_save_title))\n",
        "                f.write(\"we saved it nevertheless\")\n",
        "                rew, std = evaluate_policy(model, env, n_eval_episodes=int(1e2))\n",
        "                f.write(\"we reached {} +-{} reward score\".format(rew, std))\n",
        "                if rew < max_rew:\n",
        "                    num_training_steps = int(1e5)\n",
        "                    f.write(\"we need additional {} steps to try to reach around {} cumulative reward score\\n\".format(num_training_steps, max_rew))\n",
        "                    \n",
        "            f.write(\"it took {} min to train the {}x{} model with {}% dropout\\n\".format(round((time.process_time() - t)/60, 2), size, size, dropout*100))\n",
        "            model_save_title = \"{}.{}x{}.dropout_{}\".format(game, size, size, dropout)\n",
        "            model.save(os.path.join(PATH_NETWORKS, game, model_save_title))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOp8D1+syIhsquTVwtmyW+F",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}