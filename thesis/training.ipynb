{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOvYCtHjv6sVGxh/GZviCHa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mirklys/little-projects/blob/main/thesis/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aQd51ueWTGs",
        "outputId": "bcb17996-8883-4bf5-945c-89fafbfabde4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Apr 18 06:56:12 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cloud-tpu-client==0.10 torch==1.11.0 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.11-cp37-cp37m-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "Z7jwcbNuN3TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P24upu6YOO2t",
        "outputId": "9802b005-ef82-4aa3-bced-f6d1a014de70"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:TPU has started up successfully with version pytorch-1.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev = xm.xla_device()\n"
      ],
      "metadata": {
        "id": "NKtWpBnNOjoO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXQBSD1YOwO7",
        "outputId": "71fc31a7-5667-4eff-f84b-9ab05c44e76a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='xla', index=1)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hLt7AvqyrQOm"
      },
      "outputs": [],
      "source": [
        "!pip3 install Box2D\n",
        "!pip3 install box2d-py\n",
        "!pip3 install gym[all]\n",
        "!pip3 install gym[Box_2D]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable_baselines\n",
        "!pip install stable_baselines3"
      ],
      "metadata": {
        "id": "s-g0NopTrp47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "wytxwkxCrrVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import os\n",
        "import numpy as np\n",
        "import torch as th\n",
        "from torch import nn\n",
        "from torch.distributions.bernoulli import Bernoulli\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import time\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "from stable_baselines3.common.utils import get_device\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines.common import set_global_seeds, make_vec_env\n",
        "from stable_baselines3.common.monitor import Monitor"
      ],
      "metadata": {
        "id": "bEy3XKtCruqL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3csfnd3-rwGZ",
        "outputId": "a381c352-1970-43d4-864c-fe36befb2251"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_BASE = '/content/gdrive/MyDrive/Thesis Project'\n",
        "PATH_DATA = os.path.join(PATH_BASE, 'data/')\n",
        "PATH_NETWORKS = os.path.join(PATH_BASE, 'networks/')\n",
        "PATH_PLOTS = os.path.join(PATH_BASE, 'plots/')\n",
        "PATH_RESULTS = os.path.join(PATH_BASE, 'results/')\n",
        "PATH_LOGS = os.path.join(PATH_BASE, 'logs/')\n",
        "os.chdir(PATH_BASE)"
      ],
      "metadata": {
        "id": "21-9cWTtrx3B"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev = th.device('cuda' if th.cuda.is_available() else 'cpu')\n",
        "dev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acU7wE3MUpDT",
        "outputId": "8108abdb-d2a2-4bf5-ec06-e86f091d2734"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedMLP(BaseFeaturesExtractor):\n",
        "    def __init__(self, observation_space, dropout_param=0.0, percent_to_mask=0.0, size=32, job='train'):\n",
        "        assert job == 'train' or job == 'train_masked',\\\n",
        "            \"This class can only be initialized for jobs: train, train_masked\"\n",
        "\n",
        "        self.l1_size = size\n",
        "        self.l2_size = size\n",
        "        self.job = job\n",
        "        super(MaskedMLP, self).__init__(observation_space, self.l2_size)\n",
        "\n",
        "        self.dropout_param = dropout_param\n",
        "        self.percent_to_mask = percent_to_mask\n",
        "\n",
        "        input_size = observation_space.shape[0]\n",
        "\n",
        "        self.linear1 = nn.Linear(input_size, self.l1_size)\n",
        "        self.linear2 = nn.Linear(self.l1_size, self.l2_size)\n",
        "        self.elu = nn.ELU()\n",
        "        self.dropout = nn.Dropout(p=self.dropout_param)\n",
        "\n",
        "        if self.job == 'train':\n",
        "            self.layer1 = nn.Sequential(\n",
        "                nn.Linear(input_size, self.l1_size),\n",
        "                nn.ELU()\n",
        "            )\n",
        "            self.layer2 = nn.Sequential(\n",
        "                nn.Linear(self.l1_size, self.l2_size),\n",
        "                nn.Dropout(p=self.dropout_param),\n",
        "                nn.ELU(),\n",
        "            )\n",
        "\n",
        "        self.mask_units(self.percent_to_mask)\n",
        "\n",
        "    def mask_units(self, percent_to_mask):\n",
        "        self.mask_distribution = Bernoulli(th.tensor([1.0-percent_to_mask]*self.l2_size))   \n",
        "        self.mask = self.mask_distribution.sample()\n",
        "\n",
        "    def forward(self, observations):\n",
        "        x = self.linear1(observations)\n",
        "        x = self.elu(x)\n",
        "        x = self.linear2(x)\n",
        "        if self.job == 'train': x = self.dropout(x)\n",
        "        l2 = self.elu(x)\n",
        "\n",
        "        if not self.training or self.job == 'train_masked':\n",
        "            self.mask = self.mask.to(l2.device)\n",
        "            l2 = l2*self.mask\n",
        "\n",
        "\n",
        "        return l2\n"
      ],
      "metadata": {
        "id": "cdKKbQeqr10I"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "\"\"\"\n",
        "Training cartpole\n",
        "\"\"\"\n",
        "num_training_steps = 300000\n",
        "with open(os.path.join(PATH_LOGS, \"training_cartpole.txt\"), \"w\") as f:\n",
        "    f.write(\"training all cartpole models for 300,000 steps \\n\")\n",
        "    for size in [128, 256, 512, 1024]:\n",
        "        for dropout in [0.0, 0.2, 0.4, 0.6, 0.8]:\n",
        "            policy_kwargs = dict(\n",
        "                features_extractor_class=MaskedMLP,\n",
        "                features_extractor_kwargs=dict(dropout_param=dropout, size=size, job='train')\n",
        "            )\n",
        "            env = make_vec_env('CartPole-v1', n_envs=10, seed=0, vec_env_cls=DummyVecEnv)\n",
        "            model = PPO('MlpPolicy', env, verbose=0,\n",
        "                        policy_kwargs = policy_kwargs, device=dev)\n",
        "            t = time.process_time()\n",
        "            model.learn(num_training_steps)\n",
        "            rew, _ = evaluate_policy(model, env, n_eval_episodes=100)\n",
        "            f.write(\"we reached {} reward score\".format(rew))\n",
        "            f.write(\"it took {} min to train the {}x{} model with {}% dropout\".format(round((time.process_time() - t)/60, 2), size, size, dropout*100))\n",
        "            model_save_title = \"{}.{}x{}.dropout_{}\".format('CartPole-v1', model.policy.features_extractor.l1_size, model.policy.features_extractor.l2_size, dropout)\n",
        "            model.save(os.path.join(PATH_NETWORKS, 'CartPole-v1', model_save_title))\n",
        "            f.write('saved it')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1nLtaX6i0QxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "\"\"\"\n",
        "Training cartpole\n",
        "\"\"\"\n",
        "num_training_steps = 600000\n",
        "with open(os.path.join(PATH_LOGS, \"training_cartpole_1024_80.txt\"), \"w\") as f:\n",
        "    f.write(\"training all cartpole models for 300,000 steps \\n\")\n",
        "    for size in [1024]:\n",
        "        for dropout in [0.8]:\n",
        "            policy_kwargs = dict(\n",
        "                features_extractor_class=MaskedMLP,\n",
        "                features_extractor_kwargs=dict(dropout_param=dropout, size=size, job='train')\n",
        "            )\n",
        "            env = make_vec_env('CartPole-v1', n_envs=10, seed=0, vec_env_cls=DummyVecEnv)\n",
        "            model_save_title = \"{}.{}x{}.dropout_{}\".format('CartPole-v1', size, size, dropout)\n",
        "            model = PPO.load(os.path.join(PATH_NETWORKS, 'CartPole-v1', model_save_title))\n",
        "            model.set_env(env)\n",
        "            t = time.process_time()\n",
        "            model.learn(num_training_steps)\n",
        "            rew, _ = evaluate_policy(model, env, n_eval_episodes=100)\n",
        "            f.write(\"we reached {} reward score \\n\".format(rew))\n",
        "            f.write(\"it took {} min to train the {}x{} model with {}% dropout \\n\".format(round((time.process_time() - t)/60, 2), size, size, dropout*100))\n",
        "            model_save_title = \"{}.{}x{}.dropout_{}\".format('CartPole-v1', model.policy.features_extractor.l1_size, model.policy.features_extractor.l2_size, dropout)\n",
        "            model.save(os.path.join(PATH_NETWORKS, 'CartPole-v1', model_save_title))\n",
        "            f.write('re-saved it \\n')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "S2skSx7umaH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Training bipedal walker\n",
        "\"\"\"\n",
        "game = 'LunarLander-v2'\n",
        "with open(os.path.join(PATH_LOGS, \"training_LunarLander_128_60_80.txt\"), \"w\") as f:\n",
        "    f.write(\"Started training LunarLander models for different number of steps \\n\")\n",
        "    print(\"Started training LunarLander models for different number of steps \\n\")\n",
        "    for size in [128]:\n",
        "        for dropout in [0.8]:\n",
        "            num_training_steps = int(3e6)\n",
        "            rew = 0\n",
        "            policy_kwargs = dict(\n",
        "                features_extractor_class=MaskedMLP,\n",
        "                features_extractor_kwargs=dict(dropout_param=dropout, size=size, job='train')\n",
        "            )\n",
        "            env = make_vec_env(game, n_envs=10, seed=0, vec_env_cls=DummyVecEnv)\n",
        "            model = PPO('MlpPolicy', env, verbose=0,\n",
        "                        policy_kwargs = policy_kwargs, device=dev)\n",
        "            d = get_device()\n",
        "            print(\"device\", d)\n",
        "            f.write(\"Training {} {} model for {} steps \\n\".format( dropout, size,num_training_steps))\n",
        "            print(\"Training {} {} model for {} steps \\n\".format( dropout, size,num_training_steps))\n",
        "            t = time.process_time()\n",
        "            while rew < 300*0.85:\n",
        "                model.learn(num_training_steps)\n",
        "                model_save_title = \"{}.{}x{}.dropout_{}\".format(game, model.policy.features_extractor.l1_size, model.policy.features_extractor.l2_size, dropout)\n",
        "                model.save(os.path.join(PATH_NETWORKS, game, model_save_title))\n",
        "                f.write(\"we saved it nevertheless\")\n",
        "                print(\"we saved it nevertheless\")\n",
        "                rew, _ = evaluate_policy(model, env, n_eval_episodes=10)\n",
        "                f.write(\"we reached {} reward score\".format(rew))\n",
        "                print(\"we reached {} reward score\".format(rew))\n",
        "                if rew < 300*0.85:\n",
        "                    num_training_steps = int(1e5)\n",
        "                    f.write(\"we need additional {} steps to try to reach around 300 cumulative reward score\\n\".format(num_training_steps))\n",
        "                    print(\"we need additional {} steps to try to reach around 300 cumulative reward score\\n\".format(num_training_steps))\n",
        "                    \n",
        "            f.write(\"it took {} min to train the {}x{} model with {}% dropout\\n\".format(round((time.process_time() - t)/60, 2), size, size, dropout*100))\n",
        "            print(\"it took {} min to train the {}x{} model with {}% dropout\\n\".format(round((time.process_time() - t)/60, 2), size, size, dropout*100))\n",
        "            model_save_title = \"{}.{}x{}.dropout_{}\".format(game, model.policy.features_extractor.l1_size, model.policy.features_extractor.l2_size, dropout)\n",
        "            model.save(os.path.join(PATH_NETWORKS, game, model_save_title))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXYhlE9u0dMG",
        "outputId": "2276cdf8-be58-41f8-da4a-bf2578fe3d8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started training LunarLander models for different number of steps \n",
            "\n",
            "device cuda\n",
            "Training 0.8 128 model for 3000000 steps \n",
            "\n",
            "we saved it nevertheless\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we reached -131.77967856654723 reward score\n",
            "we need additional 100000 steps to try to reach around 300 cumulative reward score\n",
            "\n",
            "we saved it nevertheless\n",
            "we reached -126.0209431779571 reward score\n",
            "we need additional 100000 steps to try to reach around 300 cumulative reward score\n",
            "\n",
            "we saved it nevertheless\n",
            "we reached -76.39692546049575 reward score\n",
            "we need additional 100000 steps to try to reach around 300 cumulative reward score\n",
            "\n",
            "we saved it nevertheless\n",
            "we reached -194.02981819545974 reward score\n",
            "we need additional 100000 steps to try to reach around 300 cumulative reward score\n",
            "\n",
            "we saved it nevertheless\n",
            "we reached -9.182830027810468 reward score\n",
            "we need additional 100000 steps to try to reach around 300 cumulative reward score\n",
            "\n",
            "we saved it nevertheless\n",
            "we reached 11.231736828486433 reward score\n",
            "we need additional 100000 steps to try to reach around 300 cumulative reward score\n",
            "\n",
            "we saved it nevertheless\n",
            "we reached -134.66420952830646 reward score\n",
            "we need additional 100000 steps to try to reach around 300 cumulative reward score\n",
            "\n",
            "we saved it nevertheless\n",
            "we reached -304.39779602733904 reward score\n",
            "we need additional 100000 steps to try to reach around 300 cumulative reward score\n",
            "\n",
            "we saved it nevertheless\n",
            "we reached -515.0406809186592 reward score\n",
            "we need additional 100000 steps to try to reach around 300 cumulative reward score\n",
            "\n",
            "we saved it nevertheless\n",
            "we reached -242.76715203817002 reward score\n",
            "we need additional 100000 steps to try to reach around 300 cumulative reward score\n",
            "\n",
            "we saved it nevertheless\n",
            "we reached -101.12007278859947 reward score\n",
            "we need additional 100000 steps to try to reach around 300 cumulative reward score\n",
            "\n",
            "we saved it nevertheless\n",
            "we reached -150.36765923592029 reward score\n",
            "we need additional 100000 steps to try to reach around 300 cumulative reward score\n",
            "\n",
            "we saved it nevertheless\n",
            "we reached -284.3597169663757 reward score\n",
            "we need additional 100000 steps to try to reach around 300 cumulative reward score\n",
            "\n",
            "we saved it nevertheless\n",
            "we reached -134.7333195721265 reward score\n",
            "we need additional 100000 steps to try to reach around 300 cumulative reward score\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Training bipedal walker\n",
        "\"\"\"\n",
        "game = 'LunarLander-v2'\n",
        "with open(os.path.join(PATH_LOGS, \"training_LunarLander.txt\"), \"w\") as f:\n",
        "    f.write(\"Started training LunarLander models for different number of steps \\n\")\n",
        "    print(\"Started training LunarLander models for different number of steps \\n\")\n",
        "    for size in [256, 512, 1024]:\n",
        "        for dropout in [0.4, 0.6, 0.8]:\n",
        "            num_training_steps = int(1e6)\n",
        "            rew = 0\n",
        "            policy_kwargs = dict(\n",
        "                features_extractor_class=MaskedMLP,\n",
        "                features_extractor_kwargs=dict(dropout_param=dropout, size=size, job='train')\n",
        "            )\n",
        "            env = make_vec_env(game, n_envs=10, seed=0, vec_env_cls=DummyVecEnv)\n",
        "            model = PPO('MlpPolicy', env, verbose=0,\n",
        "                        policy_kwargs = policy_kwargs, device=dev, n_epochs=50, gamma=0.998)\n",
        "            #model = PPO.load(os.path.join(PATH_NETWORKS, game, \"{}.{}x{}.dropout_{}\".format(game, size, size, dropout)))\n",
        "            #model.set_env(env)\n",
        "            d = get_device()\n",
        "            print(\"device\", d)\n",
        "            f.write(\"Training {} {} model for {} steps \\n\".format( dropout, size,num_training_steps))\n",
        "            print(\"Training {} {} model for {} steps \\n\".format( dropout, size,num_training_steps))\n",
        "            t = time.process_time()\n",
        "            while rew < 300*0.9:\n",
        "                model.learn(num_training_steps)\n",
        "                model_save_title = \"{}.{}x{}.dropout_{}\".format(game, model.policy.features_extractor.l1_size, model.policy.features_extractor.l2_size, dropout)\n",
        "                model.save(os.path.join(PATH_NETWORKS, game, model_save_title))\n",
        "                f.write(\"we saved it nevertheless\")\n",
        "                print(\"we saved it nevertheless\")\n",
        "                rew, _ = evaluate_policy(model, env, n_eval_episodes=10)\n",
        "                f.write(\"we reached {} reward score\".format(rew))\n",
        "                print(\"we reached {} reward score\".format(rew))\n",
        "                if rew < 300*0.9:\n",
        "                    num_training_steps = int(1e5)\n",
        "                    f.write(\"we need additional {} steps to try to reach around 300 cumulative reward score\\n\".format(num_training_steps))\n",
        "                    print(\"we need additional {} steps to try to reach around 300 cumulative reward score\\n\".format(num_training_steps))\n",
        "                    \n",
        "            f.write(\"it took {} min to train the {}x{} model with {}% dropout\\n\".format(round((time.process_time() - t)/60, 2), size, size, dropout*100))\n",
        "            print(\"it took {} min to train the {}x{} model with {}% dropout\\n\".format(round((time.process_time() - t)/60, 2), size, size, dropout*100))"
      ],
      "metadata": {
        "id": "8jU8MOnbZwdS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}