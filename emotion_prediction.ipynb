{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "emotion_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwZZQ7_lIJWg",
        "outputId": "32041282-c17e-482f-c5e8-9f40546492d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch as th\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "NwhCc3-4e31_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAIN_PATH = '/content/gdrive/MyDrive/neuroAI/emotion prediction'\n",
        "DATA_PATH = os.path.join(MAIN_PATH, 'data')\n",
        "os.chdir(MAIN_PATH)"
      ],
      "metadata": {
        "id": "J-r9SP2efiHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotions = pd.read_csv(os.path.join(DATA_PATH, 'emotions.csv')).dropna()"
      ],
      "metadata": {
        "id": "G3Ry62-qf7kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = th.device('cuda' if th.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0oD3emwlQlN",
        "outputId": "82e82c14-fc36-4e47-b792-28470503dd7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emotions.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "utNIoA3zgREA",
        "outputId": "c8ca22f8-3ac6-4036-9fb0-c0eaf2e1f133"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  mean_d_1_a  \\\n",
              "0        4.62      30.3    -356.0     15.60      26.3       1.070       0.411   \n",
              "1       28.80      33.1      32.0     25.80      22.8       6.550       1.680   \n",
              "2        8.90      29.4    -416.0     16.70      23.7      79.900       3.360   \n",
              "3       14.90      31.6    -143.0     19.80      24.3      -0.584      -0.284   \n",
              "4       28.30      31.3      45.2     27.30      24.5      34.800      -5.790   \n",
              "5       31.00      30.9      29.6     28.50      24.0       1.650       1.540   \n",
              "6       10.80      21.0      44.7      4.87      28.1       2.140       1.020   \n",
              "7       17.80      27.8    -102.0     16.90      26.9      -3.210      -1.950   \n",
              "8       11.50      29.7      34.9     10.20      26.9     -38.000      -1.650   \n",
              "9        8.91      29.2    -314.0      6.51      30.9      -1.880       1.900   \n",
              "\n",
              "   mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_741_b  fft_742_b  fft_743_b  \\\n",
              "0      -15.70        2.06       3.150  ...      23.50       20.3       20.3   \n",
              "1        2.88        3.83      -4.820  ...     -23.30      -21.8      -21.8   \n",
              "2       90.20       89.90       2.030  ...     462.00     -233.0     -233.0   \n",
              "3        8.82        2.30      -1.970  ...     299.00     -243.0     -243.0   \n",
              "4        3.06       41.40       5.520  ...      12.00       38.1       38.1   \n",
              "5        3.83        1.87      -1.210  ...      -1.48       30.2       30.2   \n",
              "6       13.20        1.16      -4.390  ...     -15.60      -41.0      -41.0   \n",
              "7        9.80       -3.24      -0.955  ...    -177.00       32.8       32.8   \n",
              "8        3.89      -33.50      -3.300  ...      -8.38       38.7       38.7   \n",
              "9       11.90       -3.60       5.700  ...     226.00      -81.8      -81.8   \n",
              "\n",
              "   fft_744_b  fft_745_b  fft_746_b  fft_747_b  fft_748_b  fft_749_b     label  \n",
              "0      23.50    -215.00     280.00    -162.00    -162.00     280.00  NEGATIVE  \n",
              "1     -23.30     182.00       2.57     -31.60     -31.60       2.57   NEUTRAL  \n",
              "2     462.00    -267.00     281.00    -148.00    -148.00     281.00  POSITIVE  \n",
              "3     299.00     132.00     -12.40       9.53       9.53     -12.40  POSITIVE  \n",
              "4      12.00     119.00     -17.60      23.90      23.90     -17.60   NEUTRAL  \n",
              "5      -1.48     134.00       3.59     -12.70     -12.70       3.59   NEUTRAL  \n",
              "6     -15.60      89.50      40.60     -55.20     -55.20      40.60  POSITIVE  \n",
              "7    -177.00    -417.00     384.00    -186.00    -186.00     384.00  NEGATIVE  \n",
              "8      -8.38     115.00      -7.00       3.20       3.20      -7.00   NEUTRAL  \n",
              "9     226.00       1.84      99.40     -40.30     -40.30      99.40  NEGATIVE  \n",
              "\n",
              "[10 rows x 2549 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7a9b1d92-93aa-4260-8a90-718249fa81a8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th># mean_0_a</th>\n",
              "      <th>mean_1_a</th>\n",
              "      <th>mean_2_a</th>\n",
              "      <th>mean_3_a</th>\n",
              "      <th>mean_4_a</th>\n",
              "      <th>mean_d_0_a</th>\n",
              "      <th>mean_d_1_a</th>\n",
              "      <th>mean_d_2_a</th>\n",
              "      <th>mean_d_3_a</th>\n",
              "      <th>mean_d_4_a</th>\n",
              "      <th>...</th>\n",
              "      <th>fft_741_b</th>\n",
              "      <th>fft_742_b</th>\n",
              "      <th>fft_743_b</th>\n",
              "      <th>fft_744_b</th>\n",
              "      <th>fft_745_b</th>\n",
              "      <th>fft_746_b</th>\n",
              "      <th>fft_747_b</th>\n",
              "      <th>fft_748_b</th>\n",
              "      <th>fft_749_b</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.62</td>\n",
              "      <td>30.3</td>\n",
              "      <td>-356.0</td>\n",
              "      <td>15.60</td>\n",
              "      <td>26.3</td>\n",
              "      <td>1.070</td>\n",
              "      <td>0.411</td>\n",
              "      <td>-15.70</td>\n",
              "      <td>2.06</td>\n",
              "      <td>3.150</td>\n",
              "      <td>...</td>\n",
              "      <td>23.50</td>\n",
              "      <td>20.3</td>\n",
              "      <td>20.3</td>\n",
              "      <td>23.50</td>\n",
              "      <td>-215.00</td>\n",
              "      <td>280.00</td>\n",
              "      <td>-162.00</td>\n",
              "      <td>-162.00</td>\n",
              "      <td>280.00</td>\n",
              "      <td>NEGATIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>28.80</td>\n",
              "      <td>33.1</td>\n",
              "      <td>32.0</td>\n",
              "      <td>25.80</td>\n",
              "      <td>22.8</td>\n",
              "      <td>6.550</td>\n",
              "      <td>1.680</td>\n",
              "      <td>2.88</td>\n",
              "      <td>3.83</td>\n",
              "      <td>-4.820</td>\n",
              "      <td>...</td>\n",
              "      <td>-23.30</td>\n",
              "      <td>-21.8</td>\n",
              "      <td>-21.8</td>\n",
              "      <td>-23.30</td>\n",
              "      <td>182.00</td>\n",
              "      <td>2.57</td>\n",
              "      <td>-31.60</td>\n",
              "      <td>-31.60</td>\n",
              "      <td>2.57</td>\n",
              "      <td>NEUTRAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.90</td>\n",
              "      <td>29.4</td>\n",
              "      <td>-416.0</td>\n",
              "      <td>16.70</td>\n",
              "      <td>23.7</td>\n",
              "      <td>79.900</td>\n",
              "      <td>3.360</td>\n",
              "      <td>90.20</td>\n",
              "      <td>89.90</td>\n",
              "      <td>2.030</td>\n",
              "      <td>...</td>\n",
              "      <td>462.00</td>\n",
              "      <td>-233.0</td>\n",
              "      <td>-233.0</td>\n",
              "      <td>462.00</td>\n",
              "      <td>-267.00</td>\n",
              "      <td>281.00</td>\n",
              "      <td>-148.00</td>\n",
              "      <td>-148.00</td>\n",
              "      <td>281.00</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>14.90</td>\n",
              "      <td>31.6</td>\n",
              "      <td>-143.0</td>\n",
              "      <td>19.80</td>\n",
              "      <td>24.3</td>\n",
              "      <td>-0.584</td>\n",
              "      <td>-0.284</td>\n",
              "      <td>8.82</td>\n",
              "      <td>2.30</td>\n",
              "      <td>-1.970</td>\n",
              "      <td>...</td>\n",
              "      <td>299.00</td>\n",
              "      <td>-243.0</td>\n",
              "      <td>-243.0</td>\n",
              "      <td>299.00</td>\n",
              "      <td>132.00</td>\n",
              "      <td>-12.40</td>\n",
              "      <td>9.53</td>\n",
              "      <td>9.53</td>\n",
              "      <td>-12.40</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28.30</td>\n",
              "      <td>31.3</td>\n",
              "      <td>45.2</td>\n",
              "      <td>27.30</td>\n",
              "      <td>24.5</td>\n",
              "      <td>34.800</td>\n",
              "      <td>-5.790</td>\n",
              "      <td>3.06</td>\n",
              "      <td>41.40</td>\n",
              "      <td>5.520</td>\n",
              "      <td>...</td>\n",
              "      <td>12.00</td>\n",
              "      <td>38.1</td>\n",
              "      <td>38.1</td>\n",
              "      <td>12.00</td>\n",
              "      <td>119.00</td>\n",
              "      <td>-17.60</td>\n",
              "      <td>23.90</td>\n",
              "      <td>23.90</td>\n",
              "      <td>-17.60</td>\n",
              "      <td>NEUTRAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>31.00</td>\n",
              "      <td>30.9</td>\n",
              "      <td>29.6</td>\n",
              "      <td>28.50</td>\n",
              "      <td>24.0</td>\n",
              "      <td>1.650</td>\n",
              "      <td>1.540</td>\n",
              "      <td>3.83</td>\n",
              "      <td>1.87</td>\n",
              "      <td>-1.210</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.48</td>\n",
              "      <td>30.2</td>\n",
              "      <td>30.2</td>\n",
              "      <td>-1.48</td>\n",
              "      <td>134.00</td>\n",
              "      <td>3.59</td>\n",
              "      <td>-12.70</td>\n",
              "      <td>-12.70</td>\n",
              "      <td>3.59</td>\n",
              "      <td>NEUTRAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>10.80</td>\n",
              "      <td>21.0</td>\n",
              "      <td>44.7</td>\n",
              "      <td>4.87</td>\n",
              "      <td>28.1</td>\n",
              "      <td>2.140</td>\n",
              "      <td>1.020</td>\n",
              "      <td>13.20</td>\n",
              "      <td>1.16</td>\n",
              "      <td>-4.390</td>\n",
              "      <td>...</td>\n",
              "      <td>-15.60</td>\n",
              "      <td>-41.0</td>\n",
              "      <td>-41.0</td>\n",
              "      <td>-15.60</td>\n",
              "      <td>89.50</td>\n",
              "      <td>40.60</td>\n",
              "      <td>-55.20</td>\n",
              "      <td>-55.20</td>\n",
              "      <td>40.60</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>17.80</td>\n",
              "      <td>27.8</td>\n",
              "      <td>-102.0</td>\n",
              "      <td>16.90</td>\n",
              "      <td>26.9</td>\n",
              "      <td>-3.210</td>\n",
              "      <td>-1.950</td>\n",
              "      <td>9.80</td>\n",
              "      <td>-3.24</td>\n",
              "      <td>-0.955</td>\n",
              "      <td>...</td>\n",
              "      <td>-177.00</td>\n",
              "      <td>32.8</td>\n",
              "      <td>32.8</td>\n",
              "      <td>-177.00</td>\n",
              "      <td>-417.00</td>\n",
              "      <td>384.00</td>\n",
              "      <td>-186.00</td>\n",
              "      <td>-186.00</td>\n",
              "      <td>384.00</td>\n",
              "      <td>NEGATIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>11.50</td>\n",
              "      <td>29.7</td>\n",
              "      <td>34.9</td>\n",
              "      <td>10.20</td>\n",
              "      <td>26.9</td>\n",
              "      <td>-38.000</td>\n",
              "      <td>-1.650</td>\n",
              "      <td>3.89</td>\n",
              "      <td>-33.50</td>\n",
              "      <td>-3.300</td>\n",
              "      <td>...</td>\n",
              "      <td>-8.38</td>\n",
              "      <td>38.7</td>\n",
              "      <td>38.7</td>\n",
              "      <td>-8.38</td>\n",
              "      <td>115.00</td>\n",
              "      <td>-7.00</td>\n",
              "      <td>3.20</td>\n",
              "      <td>3.20</td>\n",
              "      <td>-7.00</td>\n",
              "      <td>NEUTRAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>8.91</td>\n",
              "      <td>29.2</td>\n",
              "      <td>-314.0</td>\n",
              "      <td>6.51</td>\n",
              "      <td>30.9</td>\n",
              "      <td>-1.880</td>\n",
              "      <td>1.900</td>\n",
              "      <td>11.90</td>\n",
              "      <td>-3.60</td>\n",
              "      <td>5.700</td>\n",
              "      <td>...</td>\n",
              "      <td>226.00</td>\n",
              "      <td>-81.8</td>\n",
              "      <td>-81.8</td>\n",
              "      <td>226.00</td>\n",
              "      <td>1.84</td>\n",
              "      <td>99.40</td>\n",
              "      <td>-40.30</td>\n",
              "      <td>-40.30</td>\n",
              "      <td>99.40</td>\n",
              "      <td>NEGATIVE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows Ã— 2549 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a9b1d92-93aa-4260-8a90-718249fa81a8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7a9b1d92-93aa-4260-8a90-718249fa81a8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7a9b1d92-93aa-4260-8a90-718249fa81a8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emotions['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEHbGGu4gson",
        "outputId": "a5a948a7-3b01-4c9b-c7bf-5054351582dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NEUTRAL     716\n",
              "NEGATIVE    708\n",
              "POSITIVE    708\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data(emotions: pd.DataFrame) -> tuple:\n",
        "    y = emotions['label'].replace(\n",
        "        {\n",
        "            'NEUTRAL': 1,\n",
        "            'NEGATIVE': 0,\n",
        "            'POSITIVE': 2\n",
        "        }\n",
        "    )\n",
        "    X = emotions.drop('label', axis=1)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=0)\n",
        "    return X_train.values, X_test.values, y_train.values, y_test.values"
      ],
      "metadata": {
        "id": "FYUVr2r5g90o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = data(emotions)"
      ],
      "metadata": {
        "id": "gEDu_l87hb26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFTqv7xZmzul",
        "outputId": "ad6a60f8-869f-43c8-9d38-3a949fc7f68b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  29.  ,   32.  ,   32.1 , ...,   38.7 ,   38.7 ,   -3.18],\n",
              "       [  13.2 ,   31.4 , -795.  , ..., -269.  , -269.  ,  698.  ],\n",
              "       [   7.18,   30.6 ,   26.2 , ...,  -28.1 ,  -28.1 ,  -29.6 ],\n",
              "       ...,\n",
              "       [   5.  ,   24.  , -377.  , ..., -101.  , -101.  ,  254.  ],\n",
              "       [  15.1 ,   32.4 , -146.  , ...,    5.34,    5.34,  -85.2 ],\n",
              "       [ 304.  , -112.  ,  633.  , ..., -320.  , -320.  ,  824.  ]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Data(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "    def __len__(self):\n",
        "            return len(self.y)\n",
        "    def __getitem__(self, idx):\n",
        "            return self.X[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "9rTDC5aDhd2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = Data(X_train, y_train)\n",
        "test_data = Data(X_test, y_test)"
      ],
      "metadata": {
        "id": "uGRH549ohgKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "W2c6wh-XlA5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmotionModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(EmotionModel, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(len(x), 1, -1)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.flatten(x)\n",
        "        x = nn.Linear(in_features=x.size()[1], out_features=3)(x)\n",
        "        return self.softmax(x)"
      ],
      "metadata": {
        "id": "VXDQ2fdEip8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = EmotionModel(X_train.shape[1])\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = th.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "N_EPOCHS = 100\n",
        "\n",
        "dataset_size = len(train_loader)\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
        "\n",
        "    for id_batch, (x_batch, y_batch) in enumerate(train_loader):\n",
        "        y_batch = y_batch.to(device=device, dtype=th.int64)\n",
        "        \n",
        "        y_batch_pred = model(x_batch.float())\n",
        "        \n",
        "        loss = loss_fn(y_batch_pred, y_batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if id_batch % 100 == 0:\n",
        "            loss, current = loss.item(), (id_batch + 1)* len(x_batch)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{dataset_size:>5d}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCsVmNZ-k7QR",
        "outputId": "2afe0765-983f-4f8f-d3d3-1e59ecca4471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.089301  [   32/   54]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.112937  [   32/   54]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.105464  [   32/   54]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.060115  [   32/   54]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.152818  [   32/   54]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.109928  [   32/   54]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.116979  [   32/   54]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.086081  [   32/   54]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.107259  [   32/   54]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.086468  [   32/   54]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 1.124497  [   32/   54]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 1.114032  [   32/   54]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 1.081601  [   32/   54]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 1.092453  [   32/   54]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 1.060108  [   32/   54]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 1.121121  [   32/   54]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 1.107512  [   32/   54]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 1.103273  [   32/   54]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 1.138028  [   32/   54]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 1.108989  [   32/   54]\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 1.084152  [   32/   54]\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 1.109579  [   32/   54]\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 1.107227  [   32/   54]\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 1.126651  [   32/   54]\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 1.080465  [   32/   54]\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 1.085989  [   32/   54]\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 1.095772  [   32/   54]\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 1.112066  [   32/   54]\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 1.147648  [   32/   54]\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 1.107460  [   32/   54]\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 1.126454  [   32/   54]\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 1.097262  [   32/   54]\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 1.152902  [   32/   54]\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 1.130150  [   32/   54]\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 1.076694  [   32/   54]\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 1.085068  [   32/   54]\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 1.102035  [   32/   54]\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 1.114269  [   32/   54]\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 1.078960  [   32/   54]\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 1.055383  [   32/   54]\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 1.121578  [   32/   54]\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 1.103444  [   32/   54]\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 1.082135  [   32/   54]\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 1.088344  [   32/   54]\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 1.123912  [   32/   54]\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 1.132234  [   32/   54]\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 1.128001  [   32/   54]\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 1.128566  [   32/   54]\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 1.091642  [   32/   54]\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 1.127200  [   32/   54]\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 1.104431  [   32/   54]\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 1.122336  [   32/   54]\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 1.104911  [   32/   54]\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 1.082072  [   32/   54]\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 1.131639  [   32/   54]\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 1.139063  [   32/   54]\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss: 1.126189  [   32/   54]\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss: 1.118952  [   32/   54]\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss: 1.104916  [   32/   54]\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss: 1.095179  [   32/   54]\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "loss: 1.115811  [   32/   54]\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "loss: 1.095125  [   32/   54]\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "loss: 1.126429  [   32/   54]\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "loss: 1.091203  [   32/   54]\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "loss: 1.116086  [   32/   54]\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "loss: 1.124376  [   32/   54]\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "loss: 1.117878  [   32/   54]\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "loss: 1.102662  [   32/   54]\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "loss: 1.121138  [   32/   54]\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "loss: 1.131679  [   32/   54]\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "loss: 1.082465  [   32/   54]\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "loss: 1.098635  [   32/   54]\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "loss: 1.053893  [   32/   54]\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "loss: 1.113507  [   32/   54]\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "loss: 1.066283  [   32/   54]\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "loss: 1.136503  [   32/   54]\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "loss: 1.099554  [   32/   54]\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "loss: 1.071514  [   32/   54]\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "loss: 1.062828  [   32/   54]\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "loss: 1.096519  [   32/   54]\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "loss: 1.100179  [   32/   54]\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "loss: 1.102294  [   32/   54]\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "loss: 1.111126  [   32/   54]\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "loss: 1.103085  [   32/   54]\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "loss: 1.089373  [   32/   54]\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "loss: 1.121623  [   32/   54]\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "loss: 1.072799  [   32/   54]\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "loss: 1.132486  [   32/   54]\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "loss: 1.159343  [   32/   54]\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "loss: 1.143472  [   32/   54]\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "loss: 1.102902  [   32/   54]\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "loss: 1.100607  [   32/   54]\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "loss: 1.107461  [   32/   54]\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "loss: 1.064330  [   32/   54]\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "loss: 1.101888  [   32/   54]\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "loss: 1.103863  [   32/   54]\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "loss: 1.061460  [   32/   54]\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "loss: 1.105235  [   32/   54]\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "loss: 1.126420  [   32/   54]\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "loss: 1.060977  [   32/   54]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "running_accuracy = [] \n",
        "total = 0 \n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for id_batch, (x_batch, y_batch) in enumerate(test_loader):\n",
        "    y_batch = y_batch.to(device=device, dtype=th.int64)\n",
        "    \n",
        "    y_batch_pred = model(x_batch.float())\n",
        "    \n",
        "    loss = loss_fn(y_batch_pred, y_batch)\n",
        "    y_pred = th.argmax(y_batch_pred, dim=1)\n",
        "    running_accuracy.append(th.sum(y_pred == y_batch) / y_batch.size(0))\n",
        "print('accuracy: ', sum(running_accuracy)/len(running_accuracy))"
      ],
      "metadata": {
        "id": "wtERBlQvleFZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d845e621-5130-4c3b-9927-daeb46aa80f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy:  tensor(0.3267)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        }
      ]
    }
  ]
}