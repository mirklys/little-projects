{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"Giedrius_2041025.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"PRpWmSlMNkEZ"},"source":["# **Take home assignment**\n","\n","*Giedrius* Mirklys, 2041025\n","\n","Computational Linguistics"]},{"cell_type":"markdown","metadata":{"id":"quGbVYkuOEtU"},"source":["First, let's import all required packages for this assignment."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":314},"id":"os_9Waq4OMd9","executionInfo":{"status":"ok","timestamp":1617827004023,"user_tz":-120,"elapsed":7116,"user":{"displayName":"Giedrius Mirklys","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi_y_nVA785N6RTtxpSnVfrtDhV6jbrguwxLJEB=s64","userId":"14915125409240187610"}},"outputId":"17bd8c7f-d276-4a0e-a46c-f2da4d6e1c7c"},"source":["import pandas as pd\n","import numpy as np\n","from collections import defaultdict\n","\n","#!bash # uncomment this line if you want to install the old20 package in the notebook\n","\"\"\"git clone https://github.com/stephantul/old20.git\n","git clone https://github.com/stephantul/old20.git\n","exit\n","\"\"\" # this is what you need to write in terminal\n","\n","!pip install ./old20\n","!pip install jellyfish\n","\n","from old20 import old20, old_n \n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Processing ./old20\n","Building wheels for collected packages: old20\n","  Building wheel for old20 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for old20: filename=old20-2.1.0-cp37-none-any.whl size=5473 sha256=674baeae65b6f02e239527ed4003ce4410bd45286f657b8b53761d9c2857ad31\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-fo3zj9n6/wheels/19/ab/95/1e707b7488437f542a9eaf95d85251151fd5042649abbd239d\n","Successfully built old20\n","Installing collected packages: old20\n","  Found existing installation: old20 2.1.0\n","    Uninstalling old20-2.1.0:\n","      Successfully uninstalled old20-2.1.0\n","Successfully installed old20-2.1.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["old20"]}}},"metadata":{"tags":[]}},{"output_type":"stream","text":["Requirement already satisfied: jellyfish in /usr/local/lib/python3.7/dist-packages (0.8.2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lPtz93MjObIp"},"source":["## **Task 1.**"]},{"cell_type":"code","metadata":{"id":"UCJt6FtuOgJG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617827013342,"user_tz":-120,"elapsed":738,"user":{"displayName":"Giedrius Mirklys","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi_y_nVA785N6RTtxpSnVfrtDhV6jbrguwxLJEB=s64","userId":"14915125409240187610"}},"outputId":"9fce07ec-e01f-4bf0-82bf-86cf8ad836d0"},"source":["subtlex_us = pd.read_csv(\"/content/SUBTLEXus74286wordstextversion.txt\", delimiter='\\t')\n","print(subtlex_us.head(10)) # prints first 10 entries of subtlex_us"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  Word  FREQcount  CDcount  FREQlow  Cdlow   SUBTLWF  Lg10WF  SUBTLCD  Lg10CD\n","0  the    1501908     8388  1339811   8388  29449.18  6.1766   100.00  3.9237\n","1   to    1156570     8383  1138435   8380  22677.84  6.0632    99.94  3.9235\n","2    a    1041179     8382   976941   8380  20415.27  6.0175    99.93  3.9234\n","3  you    2134713     8381  1595028   8376  41857.12  6.3293    99.92  3.9233\n","4  and     682780     8379   515365   8374  13387.84  5.8343    99.89  3.9232\n","5   it     963712     8377   685089   8370  18896.31  5.9839    99.87  3.9231\n","6    s    1057301     8377  1052788   8373  20731.39  6.0242    99.87  3.9231\n","7   of     590439     8375   573021   8372  11577.24  5.7712    99.85  3.9230\n","8  for     351650     8374   332686   8370   6895.10  5.5461    99.83  3.9230\n","9    I    2038529     8372     5147    350  39971.16  6.3093    99.81  3.9229\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"C08Q7K-LudTQ"},"source":["## **Task 2.**\n","\n","Tokens are all words in a text. However, types are distinct words in a text. So to find all the tokens I created an additional list which stores all word occurances and add the list to the tokens list."]},{"cell_type":"code","metadata":{"id":"VXbpxC-Juis0"},"source":["tokens = []\n","types = list(subtlex_us[\"Word\"])\n","for w, freq in zip(list(subtlex_us[\"Word\"]), list(subtlex_us[\"FREQcount\"])):\n","  wordL = [w]*(int(freq*0.08)) # due to lack of computational power I reduce the token list by 92%. You may uncomment the line below to use all possible tokens and do not forget to delete/comment this then\n","  #wordL = [w]*freq\n","  tokens += wordL\n","tokens = np.array(tokens)\n","types = np.array(types)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d5v9CsZ8y5vX"},"source":["## **Task 3.**"]},{"cell_type":"code","metadata":{"id":"n_87Dm1ry-5H"},"source":["class LM:\n","\n","  def __init__(self, tStr=[], ngram_size=0, k=0.01):\n","    self.tStr = tStr\n","    self.ngram_size = ngram_size\n","    self.k = k\n","    self.transition_matrix()\n","  \n","  def transition_matrix(self):\n","    self.transM = defaultdict(dict) # creating an empty nested dictionary\n","    self.ngrams = self.create_ngrams() # returns ngram tuples. See the function description for more details in the function itself\n","    for ngram in self.ngrams:\n","      history, target = ngram # splits the tuple into two variables\n","      try:\n","        self.transM[history][target] += 1 # tries to increase a subdictionary value by 1\n","      except:\n","        self.transM[history][target] = 1 # if unsuccessfully, assigns 1.\n","\n","    for key, trans in self.transM.items():\n","      # sums up all subdictionary values with smoothing. We multiply the smoothing constant k by 28 because that is how many letters in English alpabet are\n","      transition_sum = np.sum([count+self.k*28 for count in trans.values()])\n","      for letter in trans:\n","        # creates the transition probability of the main dict key and the subdict key. If we don't add the smoothing value, all the probabilities will not sum up to 1\n","        self.transM[key][letter] = (self.transM[key][letter]+self.k*28)/transition_sum \n","  \n","  def create_ngrams(self): # creates a huge list of ngrams. See create_single_ngram() for the description of how the ngram looks like\n","    ngrams = []\n","\n","    for word in self.tStr:\n","      word = list(word)\n","      bow = ['#bow#'] if self.ngram_size == 1 else ['#bow#'] * (self.ngram_size-1)\n","      word = bow+word+['#eow#']\n","      ngrams.extend(self.create_single_ngram(word))\n","    return ngrams\n","  \n","  def create_single_ngram(self, word):\n","    \"\"\"\n","    This function creates all possible self.ngram_size characters transitions of a single word with its boundaries. \n","    So, it creates an n-grams and splits into the past characters and the following character.\n","    EXAMPLE.\n","    If we have a word \"CAT\", and ngram_size = 3. The passed 'word' into this function would look like [#bow#, #bow#, C, A, T, #eow#], #bow# and #eow# mark the word's boundaries.\n","    The returned list would looke like [((#bow#, #bow#), C), ((#bow#, C), A), ((C, A), T), ((A, T), #eow#)]\n","    \"\"\"\n","    ngram = []\n","    for i in range(len(word)):\n","      try: \n","        w = []\n","        l = self.ngram_size + i\n","        w = word[i:l] # sometimes you can violate the limits of the list, therefore everything is in the try block\n","        w = (tuple(w[:-1]), w[-1]) # creates a tuple of the tuple of the past self.ngrams-1 characters and the following character.\n","        assert len(w[0]) == (self.ngram_size-1) # for some reason, the past characters tuple may not be self.ngrams-1 characters long, since I slice the 'word' list until its last value\n","        ngram.append(w)\n","      except:\n","        pass # if it is impossible to append the value, it shall remain ignored. \n","    return ngram\n","\n","  def get_ngram_probability(self, history, target): # calculates the ngrams smoothed transition probability \n","    # the code below takes into account that there might not be certain keys in transM\n","    prob = 0.0\n","    try:\n","      prob = self.transM[history][target] \n","    except KeyError:\n","      prob = self.k/28*self.k\n","        \n","    return prob \n","\n","  def perplexity(self, word): # calculates the perplexity of the given word\n","    \"\"\"\n","    Perplexity is calculated by this formula: find it please in the 40th slide from class 4.\n","    1. First we take the transition probabilities of the word from the corpus\n","    2. Take the log values of the probabilities. It must not be log2. It is a choice.\n","    3. Takes the negative average of all entropy components calculated in step 2.\n","    4. Takes the second power of the average.\n","\n","    \"\"\"\n","    probs = []\n","    bow = ['#bow#'] * (self.ngram_size-1)\n","    word = self.create_single_ngram(bow+list(word)+['#eow#']) # creates an ngram tuple list from a given word\n","    for ngram in word:\n","      probs.append(self.get_ngram_probability(ngram[0], ngram[1])) # step 1\n","                    \n","    entropy = np.log2(probs) # step 2\n","\n","    assert all(entropy <= 0) # we cant have all values of 0\n","\n","    avg_entropy = -1 * (sum(entropy) / len(entropy)) # step 3\n","    \n","    return pow(2.0, avg_entropy) # step 4\n","\n","  def generate(self, limit, generation='random'):\n","    i = 0\n","    r = 1 if self.ngram_size == 1 else self.ngram_size - 1\n","    word = ['#bow#']*r\n","    current = word[-(self.ngram_size-1):] # It needs something to start. \n","    while i < limit:\n","        letters = []\n","        probabilities = []\n","        continuations = self.transM[tuple(current)] # otherwise it could not take a subdictionary to generate the next character\n","        for l, v in continuations.items():\n","            letters.append(l)\n","            probabilities.append(v)\n","\n","        # it can generate a character in three different ways: random from the probability distribution, with the highest probability, and with the lowest probability.    \n","        if generation == 'random':\n","          new = np.random.choice(letters, size=1, p=probabilities)[0]\n","        elif generation == 'highest':\n","          new = letters[np.argmax(probabilities)]\n","        elif generation == 'lowest':\n","          new = letters[np.argmin(probabilities)] \n","        \n","        # below, it checks if it encounters the end character, or it generates words until the limit of characters.\n","\n","        if new != '#eow#': \n","            word.append(new) \n","        else: \n","            return ''.join(word[self.ngram_size-1:])\n","\n","        current = word[-(self.ngram_size-1):]   \n","        i += 1\n","\n","    return ''.join(word[self.ngram_size-1:])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lt7dTw2VU9-1"},"source":["ngram_model = LM(tStr=tokens, ngram_size=5)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rJ3iIyvnHPmm","executionInfo":{"status":"ok","timestamp":1617826526865,"user_tz":-120,"elapsed":804,"user":{"displayName":"Giedrius Mirklys","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi_y_nVA785N6RTtxpSnVfrtDhV6jbrguwxLJEB=s64","userId":"14915125409240187610"}},"outputId":"90f418e3-8acf-4627-93c1-99e1add2b4b4"},"source":["[ngram_model.generate(50) for _ in range(50)] # you can test the pentagram token model. It generates quite nice words. Tap tap on myself."],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['go',\n"," 'won',\n"," 'to',\n"," 'a',\n"," 'end',\n"," 'place',\n"," 'stop',\n"," 'the',\n"," 'looks',\n"," 'so',\n"," 'your',\n"," 'to',\n"," 'way',\n"," 'fine',\n"," 'cent',\n"," 'be',\n"," 'the',\n"," 's',\n"," 'oping',\n"," 'floor',\n"," 'you',\n"," 'over',\n"," 'big',\n"," 'asylum',\n"," 'Sooking',\n"," 'they',\n"," 'he',\n"," 'better',\n"," 'the',\n"," 'included',\n"," 't',\n"," 'law',\n"," 'our',\n"," 'who',\n"," 'you',\n"," 'morning',\n"," 'a',\n"," 'and',\n"," 'because',\n"," 'he',\n"," 'going',\n"," 'Saturiness',\n"," 'you',\n"," 'though',\n"," 'it',\n"," 'close',\n"," 'arms',\n"," 'bodies',\n"," 'shit',\n"," 'everywhere']"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"It9NF8f0GyRb"},"source":["## **Task 4**"]},{"cell_type":"code","metadata":{"id":"_8kOz7TfiM8u"},"source":["trigram_token = LM(tokens, 3)\n","trigram_type = LM(types, 3)\n","tetragram_token = LM(tokens, 4)\n","tetragram_type = LM(types, 4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r4rkOAFyHe_Y"},"source":["def lowest_perplexity(lm=LM(\"\", 0), strs=types, length=0): # calculates lowest perplexity of a certain length words\n","  p = []\n","  w2 = []\n","  for w in strs:\n","    if len(w) == length:\n","      p.append(lm.perplexity(w))\n","      w2.append(w)\n","  return (w2[p.index(min(p))], min(p)) # p.index(min(p)) outputs the index where the min(p) is."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ViRYGuqDJ0hJ"},"source":["\"\"\"\n","This cell and the cell below helps to create a nice dataframe of all 4 language models.\n","\"\"\"\n","\n","def create_dict_lowest_perplexity(lm=LM(\"\", 0), strs=types, length=0, input=\"\"):\n","  d = {}\n","  lp = lowest_perplexity(lm, strs, length)\n","  d[\"Context\"] = \"{}-gram\".format(lm.ngram_size)\n","  d[\"Input\"] = inputcustom functions\n","  d[\"Word\"] = lp[0]\n","  d[\"Perplexity\"] = lp[1]\n","\n","  return d\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":426},"id":"W2boMBvdMj53","executionInfo":{"status":"ok","timestamp":1617827211576,"user_tz":-120,"elapsed":10940,"user":{"displayName":"Giedrius Mirklys","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi_y_nVA785N6RTtxpSnVfrtDhV6jbrguwxLJEB=s64","userId":"14915125409240187610"}},"outputId":"cd1ef2e2-7ca4-4c58-f9a7-1053a6949aa4"},"source":["dd = defaultdict(list)\n","md = []\n","for l in [3, 8, 13]:\n","  md.append(create_dict_lowest_perplexity(lm=trigram_token, length=l, input='token'))\n","  md.append(create_dict_lowest_perplexity(lm=trigram_type, length=l, input='type'))\n","  md.append(create_dict_lowest_perplexity(lm=tetragram_token, length=l, input='token'))\n","  md.append(create_dict_lowest_perplexity(lm=tetragram_type, length=l, input='type'))\n","\n","for d in md:\n","    for key, value in d.items():\n","        dd[key].append(value)\n","\n","pd.DataFrame(dd)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Context</th>\n","      <th>Input</th>\n","      <th>Word</th>\n","      <th>Perplexity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3-gram</td>\n","      <td>token</td>\n","      <td>you</td>\n","      <td>3.130281</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3-gram</td>\n","      <td>type</td>\n","      <td>ing</td>\n","      <td>4.177166</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4-gram</td>\n","      <td>token</td>\n","      <td>you</td>\n","      <td>2.807220</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4-gram</td>\n","      <td>type</td>\n","      <td>man</td>\n","      <td>7.101688</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3-gram</td>\n","      <td>token</td>\n","      <td>anything</td>\n","      <td>4.682576</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>3-gram</td>\n","      <td>type</td>\n","      <td>mentions</td>\n","      <td>5.299059</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>4-gram</td>\n","      <td>token</td>\n","      <td>anything</td>\n","      <td>3.227310</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>4-gram</td>\n","      <td>type</td>\n","      <td>stations</td>\n","      <td>4.005235</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>3-gram</td>\n","      <td>token</td>\n","      <td>motherfucking</td>\n","      <td>6.241695</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>3-gram</td>\n","      <td>type</td>\n","      <td>fractionating</td>\n","      <td>5.699148</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>4-gram</td>\n","      <td>token</td>\n","      <td>backgrounders</td>\n","      <td>4.157936</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>4-gram</td>\n","      <td>type</td>\n","      <td>sexualization</td>\n","      <td>3.599976</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Context  Input           Word  Perplexity\n","0   3-gram  token            you    3.130281\n","1   3-gram   type            ing    4.177166\n","2   4-gram  token            you    2.807220\n","3   4-gram   type            man    7.101688\n","4   3-gram  token       anything    4.682576\n","5   3-gram   type       mentions    5.299059\n","6   4-gram  token       anything    3.227310\n","7   4-gram   type       stations    4.005235\n","8   3-gram  token  motherfucking    6.241695\n","9   3-gram   type  fractionating    5.699148\n","10  4-gram  token  backgrounders    4.157936\n","11  4-gram   type  sexualization    3.599976"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"sSIpUIUoSItG"},"source":["## **Task 5**"]},{"cell_type":"markdown","metadata":{"id":"uXQimwQ5j4Ld"},"source":["The cells below generate the likeliest strings of 4 different language models and creates a nice dataframe storing all asked information."]},{"cell_type":"code","metadata":{"id":"Yz_bTNVYVAeM"},"source":["def create_dict_likeliest_string(lm=LM(\"\", 0), input=\"\", size=6):\n","  d = {}\n","  w = lm.generate(size, \"highest\")\n","  d[\"Context\"] = \"{}-gram\".format(lm.ngram_size)\n","  d[\"Input\"] = input\n","  d[\"Word\"] = w\n","\n","  return d\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"id":"HyZjrtFuVp3B","executionInfo":{"status":"ok","timestamp":1617827228186,"user_tz":-120,"elapsed":621,"user":{"displayName":"Giedrius Mirklys","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi_y_nVA785N6RTtxpSnVfrtDhV6jbrguwxLJEB=s64","userId":"14915125409240187610"}},"outputId":"f2820bb6-95f4-4902-ab9e-fb57d13ea5c4"},"source":["dd = defaultdict(list)\n","md = []\n","md.append(create_dict_likeliest_string(lm=trigram_token, input='token'))\n","md.append(create_dict_likeliest_string(lm=trigram_type, input='type'))\n","md.append(create_dict_likeliest_string(lm=tetragram_token, input='token'))\n","md.append(create_dict_likeliest_string(lm=tetragram_type, input='type'))\n","\n","for d in md:\n","    for key, value in d.items():\n","        dd[key].append(value)\n","\n","pd.DataFrame(dd)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Context</th>\n","      <th>Input</th>\n","      <th>Word</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3-gram</td>\n","      <td>token</td>\n","      <td>the</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3-gram</td>\n","      <td>type</td>\n","      <td>st</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4-gram</td>\n","      <td>token</td>\n","      <td>the</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4-gram</td>\n","      <td>type</td>\n","      <td>stant</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Context  Input   Word\n","0  3-gram  token    the\n","1  3-gram   type     st\n","2  4-gram  token    the\n","3  4-gram   type  stant"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"wiUjFngqbRcC"},"source":["## **Task 6**"]},{"cell_type":"code","metadata":{"id":"KFEIiQ5u4XUc"},"source":["\"\"\"\n","Each language model generates 10 different strings. \n","Also, the levenshtein distances of 20 closest words are found for each string group.\n","\"\"\"\n","\n","token_trigram = []\n","type_trigram = []\n","token_tetragram = []\n","type_tetragram = []\n","\n","while len(token_trigram) <= 10 or len(type_trigram) <= 10 or len(token_tetragram) <= 10 or len(type_tetragram) <= 10:\n","\n","  w = trigram_token.generate(50)\n","  if w not in token_trigram and len(token_trigram) <= 10:\n","    token_trigram.append(w)\n","\n","  w = trigram_type.generate(50)\n","  if w not in type_trigram and len(type_trigram) <= 10:\n","    type_trigram.append(w)\n","\n","  w = tetragram_token.generate(50)\n","  if w not in token_tetragram and len(token_tetragram) <= 10:\n","    token_tetragram.append(w)\n","\n","  w = tetragram_type.generate(50)\n","  if w not in type_tetragram and len(type_tetragram) <= 10:\n","    type_tetragram.append(w)\n","\n","token_trigram_lev = old_n(token_trigram, types, n=20)\n","type_trigram_lev = old_n(type_trigram, types, n=20)\n","token_tetragram_lev = old_n(token_tetragram, types, n=20)\n","type_tetragram_lev = old_n(type_tetragram, types, n=20)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uxka35XS_8k7"},"source":["def create_dict_old20(g_strings=[], old20_values=[], input=\"\", ngram_s=0):\n","  d = {}\n","  d[\"N-gram size\"] = \"{}\".format(ngram_s) # let it be\n","  d[\"Input\"] = input\n","  d[\"Average OLD20 of the generated strings\"] = np.average(np.array(old20_values))\n","  d[\"Average length of the generated strings\"] = np.average(np.array([len(x) for x in g_strings]))\n","\n","  return d"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2q8Uhjx7W_nA","colab":{"base_uri":"https://localhost:8080/","height":175},"executionInfo":{"status":"ok","timestamp":1617827482813,"user_tz":-120,"elapsed":1172,"user":{"displayName":"Giedrius Mirklys","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi_y_nVA785N6RTtxpSnVfrtDhV6jbrguwxLJEB=s64","userId":"14915125409240187610"}},"outputId":"25987a90-4dc1-418d-ee12-61ee450386da"},"source":["dd = defaultdict(list)\n","md = []\n","md.append(create_dict_old20(token_trigram, token_trigram_lev, \"token\", 3))\n","md.append(create_dict_old20(type_trigram, type_trigram_lev, \"type\", 3))\n","md.append(create_dict_old20(token_tetragram, token_tetragram_lev, \"token\", 4))\n","md.append(create_dict_old20(type_tetragram, type_tetragram_lev, \"type\", 4))\n","\n","for d in md:\n","    for key, value in d.items():\n","        dd[key].append(value)\n","pd.DataFrame(dd)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Ngram size</th>\n","      <th>Input</th>\n","      <th>Average OLD20 of the generated strings</th>\n","      <th>Average length of the generated strings</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3</td>\n","      <td>token</td>\n","      <td>30.818182</td>\n","      <td>3.636364</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3</td>\n","      <td>type</td>\n","      <td>100.909091</td>\n","      <td>10.090909</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4</td>\n","      <td>token</td>\n","      <td>22.727273</td>\n","      <td>2.909091</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>type</td>\n","      <td>57.000000</td>\n","      <td>7.181818</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Ngram size  ... Average length of the generated strings\n","0          3  ...                                3.636364\n","1          3  ...                               10.090909\n","2          4  ...                                2.909091\n","3          4  ...                                7.181818\n","\n","[4 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"omiulA4uESEL"},"source":["\n","\n","* Which language model generates new strings with denser orthographic neighbourhoods (lower OLD20 values)?\n","\n","> We can see from the data frame that it is the 4-gram token model.\n","\n","\n","* How does the length of generated strings factor in? \n","\n","> Briefly, the shorter word, the fewer changes you need to make to get a wanted string. And the avrage OLD20 shows the average levenshtein distance (the min number of a single character changes) between the generated words and 20 closest neighbors in the corpus.\n","\n","* What does that tell us with respect to the different language models and their ability to capture English orthotactics?\n","\n","> Other models generate strings which are less likely to be present in the English language or to follow \"the rules\" of how English words are formed. We can see that type based models generate quite long and non-English strings. Moreover, in the English language, mojority of the most frequent words are short, so long words would less likely conform to English orthotactics. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"57CtAAM1Xy06"},"source":["## **Task 7**"]},{"cell_type":"code","metadata":{"id":"-FOQgplTX2QC"},"source":["def perplexity_calc(w):\n","  \"\"\"\n","  outputs the dictionary of perplexity values of all 4 languge models\n","  \"\"\"\n","  d = defaultdict(dict)\n","  d1, d2, d3, d4 = {}, {}, {}, {}\n","  for i in w:\n","    d1[i] = trigram_token.perplexity(i)\n","    d2[i] = trigram_type.perplexity(i)\n","    d3[i] = tetragram_token.perplexity(i)\n","    d4[i] = tetragram_type.perplexity(i)\n","  d['3 token'] = d1\n","  d['3 type'] = d2\n","  d['4 token'] = d3\n","  d['4 type'] = d4\n","  return d\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LWwCTIgJD5A5"},"source":["def create_dict_perplexity_langugage(lang='', perplexity_scores={}, words=[], rank=''):\n","  assert rank == 'highest' or rank == 'lowest' # rank here means what kind of perplexity values one wants to generate: only the highest perplexity values or only the lowest\n","  d = []\n","  for model in perplexity_scores:\n","    dd = {}\n","    dd['n-gram size'] = model.split()[0]\n","    dd['input dataset'] = words\n","    dd['concept'] = model.split()[1]\n","    dd['language'] = lang\n","    if rank == 'lowest':\n","      w = min(perplexity_scores[model])\n","    elif rank == 'highest':\n","      w = max(perplexity_scores[model])\n","    dd['word'] = w\n","    dd['perplexity'] = perplexity_scores[model][w]\n","    d.append(dd)\n","  return d"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R-Rx_sJBD5Kk"},"source":["def average_perplexity_language(lang='', perplexity_scores={}, words=[]):\n","  d = []\n","  for model in perplexity_scores:\n","    dd = {}\n","    dd['n-gram size'] = model.split()[0]\n","    dd['input dataset'] = words\n","    dd['concept'] = model.split()[1]\n","    dd['language'] = lang\n","    dd['average perplexity'] = np.average(np.array(list(perplexity_scores[model].values())))\n","    d.append(dd)\n","  return d"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AgGTHQWkQEIg"},"source":["def appendTo_DataFrame(df, x):\n","  \"\"\"\n","  Appends each value of the 'x' variable to the dataframe 'df' and returns the dataframe\n","  \"\"\"\n","  for entry in x:\n","    df = df.append(entry, ignore_index=True)\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":802},"id":"f4sYnWQ0LEls","executionInfo":{"status":"ok","timestamp":1617830042910,"user_tz":-120,"elapsed":541,"user":{"displayName":"Giedrius Mirklys","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi_y_nVA785N6RTtxpSnVfrtDhV6jbrguwxLJEB=s64","userId":"14915125409240187610"}},"outputId":"fc8a68de-f88f-49dc-8bb2-7621f2079821"},"source":["no \n","\n","eng = perplexity_calc(english_words)\n","basq = perplexity_calc(basque_words)\n","czech = perplexity_calc(czech_words)\n","dutch = perplexity_calc(dutch_words)\n","fin = perplexity_calc(finnish_words)\n","ital = perplexity_calc(italian_words)\n","\n","highest_perplexity = pd.DataFrame()\n","lowest_perplexity = pd.DataFrame()\n","average_perplexity = pd.DataFrame()\n","\n","h = [create_dict_perplexity_langugage('english', eng, english_words, 'highest'),\n","     create_dict_perplexity_langugage('basque', basq, basque_words, 'highest'),\n","     create_dict_perplexity_langugage('czech', czech, czech_words, 'highest'),\n","     create_dict_perplexity_langugage('dutch', dutch, dutch_words, 'highest'),\n","     create_dict_perplexity_langugage('finnish', fin, finnish_words, 'highest'),\n","     create_dict_perplexity_langugage('italian', ital, italian_words, 'highest')]\n","highest_perplexity = appendTo_DataFrame(highest_perplexity, h)\n","\n","l = [create_dict_perplexity_langugage('english', eng, english_words, 'lowest'),\n","     create_dict_perplexity_langugage('basque', basq, basque_words, 'lowest'),\n","     create_dict_perplexity_langugage('czech', czech, czech_words, 'lowest'),\n","     create_dict_perplexity_langugage('dutch', dutch, dutch_words, 'lowest'),\n","     create_dict_perplexity_langugage('finnish', fin, finnish_words, 'lowest'),\n","     create_dict_perplexity_langugage('italian', ital, italian_words, 'lowest')]\n","lowest_perplexity = appendTo_DataFrame(lowest_perplexity, l)\n","\n","a = [average_perplexity_language('english', eng, english_words),\n","     average_perplexity_language('basque', basq, basque_words),\n","     average_perplexity_language('czech', czech, czech_words),\n","     average_perplexity_language('dutch', dutch, dutch_words),\n","     average_perplexity_language('finnish', fin, finnish_words),\n","     average_perplexity_language('italian', ital, italian_words)]\n","average_perplexity = appendTo_DataFrame(average_perplexity, a)\n","\n","# please take off the quotation marks from the docstring below to print all the dataframes\n","\n","\"\"\"print(\"highest\", highest_perplexity, sep='\\n')\n","print('lowest', lowest_perplexity, sep='\\n')\n","print('average', average_perplexity, sep='\\n')\"\"\"\n","\n","average_perplexity\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>n-gram size</th>\n","      <th>input dataset</th>\n","      <th>concept</th>\n","      <th>language</th>\n","      <th>average perplexity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3</td>\n","      <td>[mouth, dream, sun, apple, bridge, mirror, sky...</td>\n","      <td>token</td>\n","      <td>english</td>\n","      <td>17.424964</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3</td>\n","      <td>[mouth, dream, sun, apple, bridge, mirror, sky...</td>\n","      <td>type</td>\n","      <td>english</td>\n","      <td>13.613626</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4</td>\n","      <td>[mouth, dream, sun, apple, bridge, mirror, sky...</td>\n","      <td>token</td>\n","      <td>english</td>\n","      <td>11.055962</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>[mouth, dream, sun, apple, bridge, mirror, sky...</td>\n","      <td>type</td>\n","      <td>english</td>\n","      <td>10.688955</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3</td>\n","      <td>[aho, amets, eguzki, sagar, zubi, mirail, zeru...</td>\n","      <td>token</td>\n","      <td>basque</td>\n","      <td>104.518404</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>3</td>\n","      <td>[aho, amets, eguzki, sagar, zubi, mirail, zeru...</td>\n","      <td>type</td>\n","      <td>basque</td>\n","      <td>38.089716</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>4</td>\n","      <td>[aho, amets, eguzki, sagar, zubi, mirail, zeru...</td>\n","      <td>token</td>\n","      <td>basque</td>\n","      <td>68.414639</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>4</td>\n","      <td>[aho, amets, eguzki, sagar, zubi, mirail, zeru...</td>\n","      <td>type</td>\n","      <td>basque</td>\n","      <td>33.332370</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>3</td>\n","      <td>[pusa, sen, slunce, jablko, most, zrcadlo, neb...</td>\n","      <td>token</td>\n","      <td>czech</td>\n","      <td>71.535660</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>3</td>\n","      <td>[pusa, sen, slunce, jablko, most, zrcadlo, neb...</td>\n","      <td>type</td>\n","      <td>czech</td>\n","      <td>31.156011</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>4</td>\n","      <td>[pusa, sen, slunce, jablko, most, zrcadlo, neb...</td>\n","      <td>token</td>\n","      <td>czech</td>\n","      <td>72.198331</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>4</td>\n","      <td>[pusa, sen, slunce, jablko, most, zrcadlo, neb...</td>\n","      <td>type</td>\n","      <td>czech</td>\n","      <td>35.367370</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>3</td>\n","      <td>[mond, droom, zon, appel, brug, spiegel, hemel...</td>\n","      <td>token</td>\n","      <td>dutch</td>\n","      <td>20.312645</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>3</td>\n","      <td>[mond, droom, zon, appel, brug, spiegel, hemel...</td>\n","      <td>type</td>\n","      <td>dutch</td>\n","      <td>16.563665</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>4</td>\n","      <td>[mond, droom, zon, appel, brug, spiegel, hemel...</td>\n","      <td>token</td>\n","      <td>dutch</td>\n","      <td>24.821684</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>4</td>\n","      <td>[mond, droom, zon, appel, brug, spiegel, hemel...</td>\n","      <td>type</td>\n","      <td>dutch</td>\n","      <td>16.697577</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>3</td>\n","      <td>[suu, uni, aurinko, omena, silta, peili, taiva...</td>\n","      <td>token</td>\n","      <td>finnish</td>\n","      <td>79.792833</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>3</td>\n","      <td>[suu, uni, aurinko, omena, silta, peili, taiva...</td>\n","      <td>type</td>\n","      <td>finnish</td>\n","      <td>27.494202</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>4</td>\n","      <td>[suu, uni, aurinko, omena, silta, peili, taiva...</td>\n","      <td>token</td>\n","      <td>finnish</td>\n","      <td>61.211772</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>4</td>\n","      <td>[suu, uni, aurinko, omena, silta, peili, taiva...</td>\n","      <td>type</td>\n","      <td>finnish</td>\n","      <td>35.295569</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>3</td>\n","      <td>[bocca, sogno, sole, mela, ponte, specchio, ci...</td>\n","      <td>token</td>\n","      <td>italian</td>\n","      <td>37.841784</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>3</td>\n","      <td>[bocca, sogno, sole, mela, ponte, specchio, ci...</td>\n","      <td>type</td>\n","      <td>italian</td>\n","      <td>21.123838</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>4</td>\n","      <td>[bocca, sogno, sole, mela, ponte, specchio, ci...</td>\n","      <td>token</td>\n","      <td>italian</td>\n","      <td>50.656277</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>4</td>\n","      <td>[bocca, sogno, sole, mela, ponte, specchio, ci...</td>\n","      <td>type</td>\n","      <td>italian</td>\n","      <td>23.846111</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   n-gram size  ... average perplexity\n","0            3  ...          17.424964\n","1            3  ...          13.613626\n","2            4  ...          11.055962\n","3            4  ...          10.688955\n","4            3  ...         104.518404\n","5            3  ...          38.089716\n","6            4  ...          68.414639\n","7            4  ...          33.332370\n","8            3  ...          71.535660\n","9            3  ...          31.156011\n","10           4  ...          72.198331\n","11           4  ...          35.367370\n","12           3  ...          20.312645\n","13           3  ...          16.563665\n","14           4  ...          24.821684\n","15           4  ...          16.697577\n","16           3  ...          79.792833\n","17           3  ...          27.494202\n","18           4  ...          61.211772\n","19           4  ...          35.295569\n","20           3  ...          37.841784\n","21           3  ...          21.123838\n","22           4  ...          50.656277\n","23           4  ...          23.846111\n","\n","[24 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"markdown","metadata":{"id":"nS5C4Za_LZEM"},"source":["\n","\n","> What can you say about relations between the target languages and English based on how perplex the language models are when fed with translations of these 10 core concepts? \n","\n","To answer the question, I shall refer to the average perplexity table as it represents the scores in the best way. Obsiously, the English language's perplexity scores are the lowest. Therefore, they can be base values for comparing perplexities of different languages. As it could be foreseen beforehand, the Dutch language is the most similar to English since the perplexity values are the closest. Moreover, the Basque language is very distinct from English language. Fascinatingly, Italian orthotactics are no so different from English ones'. Though, you can find very similar words to English in Italian indeed. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"Z0nfLvGxO0B9"},"source":["### **Some notes for the grader**\n","\n","Python is a pretty self-explanatory language, and I do believe I should not explain each line. For example, when I add something to a list or create a simple dictionary. \n","\n","Also, some lines are indented with 2 spaces, some are with 4. Please do not be bothered by it. If for some reason it does not work, you are free to edit it. But as far as I know, we are allowed to use either 2-space or 4-space indentation. Perhaps it depends on the version. \n","\n","Finally, I spotted that I get an inappropriate word as an output. So I am not the one who you should blame here. Blame the data :)"]}]}